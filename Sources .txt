Pract 1
!pip install pipeline
from transformers import pipeline
# Load a sentiment-analysis pipeline
sentiment_analyzer = pipeline('sentiment-analysis')
def analyze_sentiment(text):
# Analyze sentiment
result = sentiment_analyzer(text)
return result
if __name__ == "_main_":
text = "I love using natural language processing to analyze text 
data!"
sentiment = analyze_sentiment(text)
print(sentiment)

Pract 2

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
model_name = "microsoft/DialoGPT-medium"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained (model_name)
def chat_with_bot(input_text, history=[]):
new_user_input_ids = tokenizer.encode(input_text + tokenizer.eos_token, 
return_tensors='pt')
bot_input_ids = torch.cat([torch.cat(history, dim=-1), 
new_user_input_ids], dim=-1) if history else new_user_input_ids
chat_history_ids = model.generate (bot_input_ids, max_length=1000, 
pad_token_id=tokenizer.eos_token_id)
response = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-
1]:][0], skip_special_tokens=True)
history.append(bot_input_ids)
return response, history
history = []
while True:
user_input = input("You: ")
if user_input.lower() == "exit":
break
response, history = chat_with_bot(user_input, history)
print(f"Bot: {response}")

Pract 3

import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Embedding, Flatten, Input, Dense, 
Concatenate, Dropout
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
# 1. Load the MovieLens dataset
url = "https://files.grouplens.org/datasets/movielens/ml-latest-
small.zip"
data_path = tf.keras.utils.get_file("ml-latest-small.zip", url, 
extract=True)
ratings_file = data_path.replace("ml-latest-small.zip", "ml-latest-
small/ratings.csv")
ratings = pd.read_csv(ratings_file)
# 2. Preprocess the data
# Encode userId and movieId as integers
user_encoder = LabelEncoder()
item_encoder = LabelEncoder()
ratings['userId'] = user_encoder.fit_transform(ratings['userId'])
ratings['movieId'] = item_encoder.fit_transform(ratings['movieId'])
num_users = ratings['userId'].nunique()
num_movies = ratings['movieId'].nunique()
# Prepare train and test datasets
X = ratings[['userId', 'movieId']].values
y = ratings['rating'].values
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
random_state=42)
# 3. Define the collaborative filtering model
def build_model(num_users, num_movies, embedding_dim=50):
# User embedding
user_input = Input(shape=(1,), name="user_input")
user_embedding = Embedding(input_dim=num_users, 
output_dim=embedding_dim, name="user_embedding")(user_input)
user_vector = Flatten()(user_embedding)
 # Movie embedding
movie_input = Input(shape=(1,), name="movie_input")
movie_embedding = Embedding(input_dim=num_movies, 
output_dim=embedding_dim, name="movie_embedding")(movie_input)
movie_vector = Flatten()(movie_embedding)
# Concatenate user and movie embeddings
concat = Concatenate()([user_vector, movie_vector])
dense = Dense(128, activation='relu')(concat)
dense = Dropout(0.3)(dense)
dense = Dense(64, activation='relu')(dense)
dense = Dropout(0.3)(dense)
output = Dense(1)(dense) # Predict rating
model = Model(inputs=[user_input, movie_input], outputs=output)
return model
# Build the model
embedding_dim = 50
model = build_model(num_users, num_movies, embedding_dim)
model.compile(optimizer='adam', loss='mse', metrics=['mae'])
# 4. Train the model
history = model.fit(
[X_train[:, 0], X_train[:, 1]], y_train,
validation_data=([X_test[:, 0], X_test[:, 1]], y_test),
epochs=10,
batch_size=64
)
# 5. Evaluate the model
loss, mae = model.evaluate([X_test[:, 0], X_test[:, 1]], y_test)
print(f"Test MAE: {mae:.4f}")
# 6. Make predictions
user_id = 15 # Example: A valid user ID from the dataset
movie_id = 17 # Example: A valid movie ID from the dataset
# Make a prediction
predicted_rating = model.predict([np.array([user_id]), 
np.array([movie_id])])
print(f"Predicted rating for user {user_id} and movie {movie_id}: 
{predicted_rating[0][0]:.2f}")

Pract 4

!pip install diffusers
from diffusers import StableDiffusionPipeline
import torch
model_id = "runwayml/stable-diffusion-v1-5"
pipe = StableDiffusionPipeline.from_pretrained(model_id, 
torch_dtype=torch.float32)
prompt = "A futuristic cityscape at sunset with flying cars"
image = pipe(prompt).images[0]
image.save(f"generated_image{prompt}.png")

Pract 5

import numpy as np
grid_size = 5
episodes = 500
alpha = 0.1
gamma = 0.9
epsilon = 0.2
q_table = np.zeros((grid_size, grid_size, 4))
goal = (4, 4)
obstacles = [(1, 1), (2, 2), (3, 3)]
actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]
def is_valid(x, y):
return 0 <= x < grid_size and 0 <= y < grid_size and (x, y) not in
obstacles
def choose_action(x, y):
if np.random.uniform() < epsilon:
return np.random.randint(4)
return np.argmax(q_table[x, y])
for _ in range(episodes):
x, y = 0, 0
while (x, y) != goal:
action = choose_action(x, y)
dx, dy = actions[action]
new_x, new_y = x + dx, y + dy
if not is_valid(new_x, new_y):
new_x, new_y = x, y
reward = -1
elif (new_x, new_y) == goal:
reward = 100
else:
reward = -0.1
q_table[x, y, action] += alpha * (reward + gamma * 
np.max(q_table[new_x, new_y]) - q_table[x, y, action])
x, y = new_x, new_y
if (x, y) == goal:
break
x, y = 0, 0
path = [(x, y)]
while (x, y)!= goal:
action = np.argmax(q_table[x, y])
dx, dy = actions[action]
x, y = x + dx, y + dy
path.append((x, y))
if not is_valid(x, y):
break
print("Learned path:",path)

Pract 7a

# Code A: Using TPOT for Hyperparameter Tuning
#Install TPOT
#!pip install tpot
from tpot import TPOTClassifier
from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris
# Load dataset
data = load_iris()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, 
random_state=42)
# Initialize TPOT for evolutionary optimization
tpot = TPOTClassifier(
generations=5, # Number of generations to evolve
population_size=20, # Number of individuals in each generation
verbosity=2, # Output progress updates
random_state=42,
n_jobs=-1 # Use all processors
)
# Run optimization
tpot.fit(X_train, y_train)
# Evaluate the best model
print("Test Accuracy:", tpot.score(X_test, y_test))
# Export the optimized pipeline as Python code
tpot.export("best_pipeline.py")

Pract 7b

#code b: Using Optuna for Bayesian Optimization
import optuna
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import cross_val_score
# Suppress Optuna logging
optuna.logging.set_verbosity(optuna.logging.WARNING)
# Load dataset
data = load_iris()
X, y = data.data, data.target
# Objective function for optimization
def objective(trial):
n_estimators = trial.suggest_int('n_estimators', 10, 200)
max_depth = trial.suggest_int('max_depth', 1, 32)
min_samples_split = trial.suggest_int('min_samples_split', 2, 10)
model = RandomForestClassifier(
n_estimators=n_estimators,
max_depth=max_depth,
min_samples_split=min_samples_split,
random_state=42
)
scores = cross_val_score(model, X, y, cv=3, scoring='accuracy')
return scores.mean()
# Create and run study
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=50)
# Print only the desired output
print("Best parameters:", study.best_params)
print("Best cross-validation accuracy:", study.best_value)